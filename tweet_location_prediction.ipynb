{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Tweets\n",
    "\n",
    "This project involves the use a Naive Bayes Classifier to find patterns in real tweets. There are three files: `new_york.json`, `london.json`, and `paris.json`. These three files contain tweets that are gathered from those locations.\n",
    "\n",
    "The goal is to create a classification algorithm that can classify any tweet (or sentence) and predict whether that sentence came from New York, London, or Paris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate the Data\n",
    "\n",
    "To begin, let's take a look at the data. I've imported the datasets and printed the following information:\n",
    "* The number of tweets.\n",
    "* The columns, or features, of a tweet.\n",
    "* The text of the 12th tweet in the New York dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4723\n",
      "Index(['created_at', 'id', 'id_str', 'text', 'display_text_range', 'source',\n",
      "       'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
      "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
      "       'contributors', 'is_quote_status', 'quote_count', 'reply_count',\n",
      "       'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted',\n",
      "       'filter_level', 'lang', 'timestamp_ms', 'extended_tweet',\n",
      "       'possibly_sensitive', 'quoted_status_id', 'quoted_status_id_str',\n",
      "       'quoted_status', 'quoted_status_permalink', 'extended_entities',\n",
      "       'withheld_in_countries'],\n",
      "      dtype='object')\n",
      "Be best #ThursdayThoughts\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "new_york_tweets = pd.read_json(\"new_york.json\", lines=True)\n",
    "print(len(new_york_tweets))\n",
    "print(new_york_tweets.columns)\n",
    "print(new_york_tweets.loc[12][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5341\n",
      "Index(['created_at', 'id', 'id_str', 'text', 'display_text_range', 'source',\n",
      "       'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
      "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
      "       'contributors', 'is_quote_status', 'extended_tweet', 'quote_count',\n",
      "       'reply_count', 'retweet_count', 'favorite_count', 'entities',\n",
      "       'favorited', 'retweeted', 'filter_level', 'lang', 'timestamp_ms',\n",
      "       'possibly_sensitive', 'quoted_status_id', 'quoted_status_id_str',\n",
      "       'quoted_status', 'quoted_status_permalink', 'extended_entities'],\n",
      "      dtype='object')\n",
      "I saw this on the BBC and thought you should see it:\n",
      "\n",
      "The precious metal sparking a new gold rush - https://t.co/ScW4MOSobZ\n"
     ]
    }
   ],
   "source": [
    "london_tweets = pd.read_json(\"london.json\", lines=True)\n",
    "print(len(london_tweets))\n",
    "print(london_tweets.columns)\n",
    "print(london_tweets.loc[12][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2510\n",
      "Index(['created_at', 'id', 'id_str', 'text', 'source', 'truncated',\n",
      "       'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
      "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
      "       'contributors', 'is_quote_status', 'quote_count', 'reply_count',\n",
      "       'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted',\n",
      "       'filter_level', 'lang', 'timestamp_ms', 'display_text_range',\n",
      "       'extended_entities', 'possibly_sensitive', 'quoted_status_id',\n",
      "       'quoted_status_id_str', 'quoted_status', 'quoted_status_permalink',\n",
      "       'extended_tweet'],\n",
      "      dtype='object')\n",
      "Hauts-de-Seine : l’incendie d’Issy-les-Moulineaux prive aussi 16 000 foyers de courant https://t.co/Hlb02Fpliy\n"
     ]
    }
   ],
   "source": [
    "paris_tweets = pd.read_json(\"paris.json\", lines=True)\n",
    "print(len(paris_tweets))\n",
    "print(paris_tweets.columns)\n",
    "print(paris_tweets.loc[12][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'first', 'glance', 'it', 'looked', 'like', 'asparagus', 'with', 'chicken', 'and', 'gravy', 'smothered', 'over', 'it', 'or', 'potatoes', 'she', 'got', 'ta', 'be', 'extra', 'https']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def list_tokenizer(list_of_text):\n",
    "    tokenized_list = []\n",
    "    updated_tokenized_list = []\n",
    "    for text in list_of_text:\n",
    "        tokenized_list.append(word_tokenize(text))\n",
    "    for text in tokenized_list:\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word.isalpha():\n",
    "                new_text.append(word.lower())\n",
    "        updated_tokenized_list.append(new_text)\n",
    "    return updated_tokenized_list\n",
    "\n",
    "new_york_tokenized_text = list_tokenizer(new_york_tweets.text.tolist())\n",
    "london_tokenized_text = list_tokenizer(london_tweets.text.tolist())\n",
    "paris_tokenized_text = list_tokenizer(paris_tweets.text.tolist())\n",
    "\n",
    "print(new_york_tokenized_text[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first',\n",
       " 'glance',\n",
       " 'looked',\n",
       " 'like',\n",
       " 'asparagus',\n",
       " 'chicken',\n",
       " 'gravy',\n",
       " 'smothered',\n",
       " 'potatoes',\n",
       " 'got',\n",
       " 'ta',\n",
       " 'extra',\n",
       " 'https']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stop_words(tokenized_list, language):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    filtered = []\n",
    "    for text in tokenized_list:\n",
    "        new_list = [word for word in text if not word in stop_words]\n",
    "        filtered.append(new_list)\n",
    "    return filtered\n",
    "\n",
    "new_york_important_text = remove_stop_words(new_york_tokenized_text, 'english')\n",
    "london_important_text = remove_stop_words(london_tokenized_text, 'english')\n",
    "paris_important_text = remove_stop_words(paris_tokenized_text, 'french')\n",
    "\n",
    "new_york_important_text[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Using the SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first',\n",
       " 'glanc',\n",
       " 'look',\n",
       " 'like',\n",
       " 'asparagus',\n",
       " 'chicken',\n",
       " 'gravi',\n",
       " 'smother',\n",
       " 'potato',\n",
       " 'got',\n",
       " 'ta',\n",
       " 'extra',\n",
       " 'https']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def stem(tokenized_list, language):\n",
    "    porter = SnowballStemmer(language)\n",
    "    new_list = []\n",
    "    for text in tokenized_list:\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            new_text.append(porter.stem(word))\n",
    "        new_list.append(new_text)\n",
    "    return new_list\n",
    "\n",
    "new_york_stemmed = stem(new_york_important_text, 'english')\n",
    "london_stemmed = stem(london_important_text, 'english')\n",
    "paris_stemmed = stem(paris_important_text, 'french')\n",
    "new_york_stemmed[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying using language: Naive Bayes Classifier\n",
    "\n",
    "I'm going to create a Naive Bayes Classifier! I will begin by looking at the way language is used differently in these three locations. I will grab the text of all of the tweets and make it one big list.I will also make the labels associated with those tweets. `0` represents a New York tweet, `1`  represents a London tweet, and `2` represents a Paris tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = new_york_stemmed + london_stemmed + paris_stemmed\n",
    "labels = [0] * len(new_york_important_text) + [1] * len(london_important_text) + [2] * len(paris_important_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_updated = []\n",
    "for tweet in all_tweets:\n",
    "    new_tweet = \" \".join(tweet)\n",
    "    all_tweets_updated.append(new_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10059 2515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(all_tweets_updated, labels, test_size=0.2, random_state=1)\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Count Vectors\n",
    "\n",
    "To use a Naive Bayes Classifier, I need to transform the lists of words into count vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say bye hard especi your say bye comfort\n",
      "  (0, 14107)\t2\n",
      "  (0, 14110)\t1\n",
      "  (0, 14111)\t1\n",
      "  (0, 14112)\t1\n",
      "  (0, 19246)\t1\n",
      "  (0, 30860)\t1\n",
      "  (0, 30876)\t1\n",
      "  (0, 30877)\t1\n",
      "  (0, 43646)\t1\n",
      "  (0, 43655)\t1\n",
      "  (0, 43656)\t1\n",
      "  (0, 88461)\t2\n",
      "  (0, 88489)\t2\n",
      "  (0, 88490)\t1\n",
      "  (0, 88491)\t1\n",
      "  (0, 115969)\t1\n",
      "  (0, 116002)\t1\n",
      "  (0, 116003)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "counter = CountVectorizer(ngram_range = (1, 3))\n",
    "counter.fit(train_data)\n",
    "train_counts = counter.transform(train_data)\n",
    "test_counts = counter.transform(test_data)\n",
    "print(train_data[3])\n",
    "print(train_counts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_counts, train_labels)\n",
    "predictions = classifier.predict(test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Your Model\n",
    "\n",
    "I will evauluate the classifier by printing the accuracy score and confussion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 72.25%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f'The accuracy is: {round((accuracy * 100), 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[566 403   4]\n",
      " [225 828   8]\n",
      " [ 11  47 423]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tweet is from Paris\n"
     ]
    }
   ],
   "source": [
    "def predict(tweet):\n",
    "    token = word_tokenize(tweet)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_token_1 = [word.lower() for word in token if not word in stop_words]\n",
    "    new_token_2 = [SnowballStemmer('english').stem(word) for word in new_token_1]\n",
    "    new_token_3 = \" \".join(new_token_2)\n",
    "    tweet_counts = counter.transform([new_token_3])\n",
    "    result = (classifier.predict(tweet_counts))\n",
    "    if result[0] == 0:\n",
    "        return \"This tweet is from New York\"\n",
    "    elif result[0] == 1:\n",
    "        return \"This tweet is from London\"\n",
    "    else:\n",
    "        return \"This tweet is from Paris\"\n",
    "    \n",
    "print(predict(\"Pouvez-vous parlez plus lentement s’il vous plaît?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
